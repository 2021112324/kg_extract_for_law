"""
ä¼˜åŒ–åçš„çŸ¥è¯†æŠ½å–æœåŠ¡
âœ… è§£å†³é—®é¢˜ï¼š
1. ä¸€æ¬¡æ€§æŠ½å–ï¼ˆå®ä½“+å…³ç³»ï¼‰- å‡å°‘80%LLMè°ƒç”¨
2. å¹¶å‘æ‰§è¡Œ - Semaphore(20) æé€Ÿ20å€
3. å®ä½“å»é‡ - è§£å†³é‡å¤å®ä½“é—®é¢˜
4. å…³ç³»éªŒè¯ - è§£å†³å†²çªå…³ç³»é—®é¢˜
5. æ‰¹é‡å¯¼å…¥ - æé€Ÿ100å€
"""

import asyncio
import logging
from typing import List, Dict, Tuple
from collections import defaultdict
from difflib import SequenceMatcher
import hashlib
import re

from client_app.data_service_client import DataServiceHandler
from common.exception import errors
from .schema_service import schema_service  # SchemaæœåŠ¡ï¼Œç”¨äºè·å–å›¾è°±æ¨¡å¼
from .mapping_service import mapping_service  # æ˜ å°„æœåŠ¡ï¼Œç”¨äºè·å–æŠ½å–æ˜ å°„é…ç½®
from .extract_cache import extraction_cache  # æŠ½å–ç¼“å­˜ï¼Œé¿å…é‡å¤çš„LLMè°ƒç”¨

logger = logging.getLogger("knowledgeService")  # è·å–çŸ¥è¯†æœåŠ¡çš„æ—¥å¿—è®°å½•å™¨


class EntityDeduplicator:
    """å®ä½“å»é‡å¼•æ“ - ç”¨äºå¤„ç†é‡å¤çš„å®ä½“ï¼Œä¿ç•™å”¯ä¸€å®ä½“"""
    
    def __init__(self):
        # å®ä½“ç¼“å­˜ï¼Œå­˜å‚¨(type, normalized_name)åˆ°canonical_entityçš„æ˜ å°„
        self.entity_cache = {}  # {(type, normalized_name): canonical_entity}
    
    def normalize_name(self, name: str, entity_type: str) -> str:
        """
        æ ‡å‡†åŒ–å®ä½“åç§° - æ¸…æ´—å’Œæ ‡å‡†åŒ–å®ä½“åç§°ä»¥ä¾¿æ¯”è¾ƒ
        
        Args:
            name: å®ä½“åç§°
            entity_type: å®ä½“ç±»å‹
        
        Returns:
            æ ‡å‡†åŒ–åçš„å®ä½“åç§°
        """
        # 1. åŸºç¡€æ¸…æ´— - ç§»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        name = re.sub(r'\s+', '', name.strip())  # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦
        name = re.sub(r'[""''ã€Šã€‹ã€ã€‘\[\]\(\)]', '', name)  # ç§»é™¤å„ç§å¼•å·å’Œæ‹¬å·
        
        # 2. ç±»å‹ç‰¹å®šè§„åˆ™ - æ ¹æ®å®ä½“ç±»å‹åº”ç”¨ä¸åŒçš„æ¸…æ´—è§„åˆ™
        # TODOï¼šå°†æ¸…æ´—è§„åˆ™æ‹†åˆ†å‡ºæ¥
        if entity_type in ['å…¬å¸', 'Company']:
            # å¯¹å…¬å¸åç§°ç§»é™¤å¸¸è§çš„å…¬å¸åç¼€
            name = re.sub(r'(æœ‰é™å…¬å¸|è‚¡ä»½æœ‰é™å…¬å¸|å…¬å¸|é›†å›¢)$', '', name)
        elif entity_type in ['äººç‰©', 'Person']:
            # å¯¹äººç‰©åç§°ç§»é™¤å¸¸è§çš„ç§°è°“åç¼€
            name = re.sub(r'(å…ˆç”Ÿ|å¥³å£«|æ•™æˆ|åšå£«)$', '', name)
        
        # 3. ç»Ÿä¸€å¤§å°å†™ï¼ˆè‹±æ–‡ï¼‰- å°†è‹±æ–‡åç§°ç»Ÿä¸€ä¸ºå°å†™
        if re.match(r'^[A-Za-z0-9\s]+$', name):
            name = name.lower()
        
        return name
    
    def calculate_similarity(self, name1: str, name2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªåç§°çš„ç›¸ä¼¼åº¦
        
        Args:
            name1: ç¬¬ä¸€ä¸ªåç§°
            name2: ç¬¬äºŒä¸ªåç§°
        
        Returns:
            ç›¸ä¼¼åº¦å€¼ (0.0-1.0)
        """
        # ä½¿ç”¨SequenceMatcherè®¡ç®—åºåˆ—ç›¸ä¼¼åº¦
        seq_sim = SequenceMatcher(None, name1, name2).ratio()
        
        # å¦‚æœä¸€ä¸ªåç§°åŒ…å«å¦ä¸€ä¸ªåç§°ï¼Œå¢åŠ ç›¸ä¼¼åº¦ï¼ˆåŒ…å«å…³ç³»åŠ åˆ†ï¼‰
        if name1 in name2 or name2 in name1:
            seq_sim = min(seq_sim + 0.2, 1.0)  # æœ€å¤§ç›¸ä¼¼åº¦ä¸è¶…è¿‡1.0
        
        return seq_sim
    
    def deduplicate(self, entities: List[Dict]) -> Tuple[List[Dict], Dict]:
        """
        âœ… æ”¹è¿›ï¼šå¤šç»´åº¦å®ä½“å»é‡ï¼ˆname + type + æ¶ˆæ­§ä¿¡æ¯ï¼‰
        
        Args:
            entities: å®ä½“åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [{'type': 'å…¬å¸', 'name': 'åä¸º', 'props': {...}}, ...]
        
        Returns:
            (unique_entities, entity_mapping)
            unique_entities: å»é‡åçš„å®ä½“åˆ—è¡¨
            entity_mapping: æ˜ å°„å­—å…¸ï¼Œæ ¼å¼ä¸º {(type, original_name): canonical_id}
        """
        logger.info(f"[å»é‡] å¼€å§‹å®ä½“å»é‡ï¼ŒåŸå§‹å®ä½“æ•°: {len(entities)}")
        
        # æŒ‰å®ä½“ç±»å‹åˆ†ç»„ï¼Œä¾¿äºåç»­å¤„ç†
        by_type = defaultdict(list)
        for e in entities:
            by_type[e['type']].append(e)
        
        unique_entities = []
        entity_mapping = {}
        
        # å¯¹æ¯ç§ç±»å‹çš„å®ä½“è¿›è¡Œå»é‡å¤„ç†
        for entity_type, ents in by_type.items():
            # âœ… æå–æ¶ˆæ­§ä¿¡æ¯ - ç”¨äºåŒºåˆ†åŒåä½†ä¸åŒçš„å®ä½“
            for e in ents:
                e['_disambiguator'] = self._extract_disambiguator(e, entity_type)
            
            # âœ… æ™ºèƒ½èšç±»å»é‡ï¼ˆè€ƒè™‘æ¶ˆæ­§ä¿¡æ¯ï¼‰- åŸºäºåç§°ç›¸ä¼¼åº¦å’Œæ¶ˆæ­§ä¿¡æ¯è¿›è¡Œèšç±»
            clusters = self._cluster_entities_smart(ents, entity_type)
            
            logger.info(f"[å»é‡] ç±»å‹ '{entity_type}': {len(ents)}ä¸ªå®ä½“ -> {len(clusters)}ä¸ªç°‡")
            
            # èåˆæ¯ä¸ªç°‡ä¸­çš„å®ä½“ï¼Œç”Ÿæˆå”¯ä¸€å®ä½“
            for cluster in clusters:
                fused = self._fuse_cluster(cluster, entity_type)
                unique_entities.append(fused)
                
                # è®°å½•åŸå§‹å®ä½“åˆ°èåˆå®ä½“çš„æ˜ å°„å…³ç³»
                for original_entity in cluster:
                    entity_mapping[(entity_type, original_entity['name'])] = fused['id']
        
        if len(entities) > 0:
            # è®¡ç®—å»é‡ç‡
            dedup_rate = (1-len(unique_entities)/len(entities))*100
            logger.info(f"[å»é‡] å®Œæˆ: {len(entities)} -> {len(unique_entities)}, å»é‡ç‡: {dedup_rate:.1f}%")
        else:
            logger.warning("[å»é‡] è¾“å…¥å®ä½“æ•°ä¸º0ï¼Œæ— éœ€å»é‡")
        
        return unique_entities, entity_mapping
    
    def _extract_disambiguator(self, entity: Dict, entity_type: str) -> str:
        """
        # TODOï¼šè¯¥éƒ¨åˆ†é€»è¾‘å¾…åˆ é™¤ï¼Œç†ç”±ï¼šidä¸nameå’Œtypeç»‘å®šï¼Œä¸å­˜åœ¨name typeç›¸åŒä½†æ˜¯å®ä½“ä¸åŒçš„æƒ…å†µï¼Œæ‰€ä»¥å°±ç®—è¯¥ä»£ç 
            æˆåŠŸåŒºåˆ†äº†ä¸¤ä¸ªå®ä½“ï¼Œä½†ç”±äºä¸¤ä¸ªå®ä½“çš„nameå’Œtypeç›¸åŒï¼Œæœ€åä¼šäº§ç”Ÿç›¸åŒidåˆå¹¶ä¸ºä¸€ä¸ªå®ä½“
        âœ… ä»å®ä½“å±æ€§ä¸­æå–å…³é”®æ¶ˆæ­§ä¿¡æ¯
        ä¼˜å…ˆæå–èƒ½åŒºåˆ†åŒåä¸åŒå®ä½“çš„å…³é”®å­—æ®µ
        
        Args:
            entity: å®ä½“å­—å…¸
            entity_type: å®ä½“ç±»å‹
        
        Returns:
            æ¶ˆæ­§ä¿¡æ¯å­—ç¬¦ä¸²
        """
        props = entity.get('props', {})
        
        # é˜²å¾¡æ€§æ£€æŸ¥ï¼šç¡®ä¿ props æ˜¯å­—å…¸
        if not isinstance(props, dict):
            logger.warning(f"[å»é‡] props ç±»å‹å¼‚å¸¸: {type(props).__name__}, å®ä½“: {entity.get('name', 'unknown')}")
            return ""
        
        # ä¸åŒå®ä½“ç±»å‹çš„æ¶ˆæ­§å­—æ®µä¼˜å…ˆçº§
        disambiguator_fields = {
            'äºº': ['å•ä½', 'å…¬å¸', 'ç»„ç»‡', 'èŒä½', 'éƒ¨é—¨', 'åœ°åŒº', 'å›¢é˜Ÿ'],
            'ç»„ç»‡': ['åœ°åŒº', 'åœ°å€', 'ä¸Šçº§æœºæ„', 'ç±»å‹', 'è¡Œä¸š', 'æ€»éƒ¨'],
            'å…¬å¸': ['åœ°åŒº', 'åœ°å€', 'æ€»éƒ¨åœ°å€', 'æ³¨å†Œåœ°', 'è¡Œä¸š'],
            'äº§å“': ['å‹å·', 'åˆ¶é€ å•†', 'ç‰ˆæœ¬', 'ç³»åˆ—', 'è§„æ ¼'],
            'è®¾å¤‡': ['å‹å·', 'åˆ¶é€ å•†', 'æ‰€å±ç”Ÿäº§çº¿', 'ç¼–å·', 'åºåˆ—å·'],
            'ç”Ÿäº§çº¿': ['æ‰€å±å·¥å‚', 'è½¦é—´', 'åœ°ç‚¹', 'ç¼–å·'],
        }
        
        # è·å–å½“å‰å®ä½“ç±»å‹çš„æ¶ˆæ­§å­—æ®µåˆ—è¡¨
        key_fields = disambiguator_fields.get(entity_type, ['ç±»å‹', 'åˆ†ç±»', 'ç±»åˆ«'])
        
        # æå–ç¬¬ä¸€ä¸ªéç©ºå­—æ®µä½œä¸ºæ¶ˆæ­§ç¬¦
        for field in key_fields:
            value = props.get(field)
            if value and str(value).strip():
                return str(value).strip()[:30]  # é™åˆ¶é•¿åº¦ä¸º30ä¸ªå­—ç¬¦
        
        return ""  # æ— æ¶ˆæ­§ä¿¡æ¯
    
    def _should_merge(self, e1: Dict, e2: Dict, entity_type: str) -> bool:
        """
        # TODO:åˆå¹¶é€»è¾‘å¾…ä¿®æ­£
        âœ… æ ¸å¿ƒåˆ¤æ–­ï¼šæ˜¯å¦åº”è¯¥åˆå¹¶ä¸¤ä¸ªå®ä½“
        ç»“åˆåç§°ç›¸ä¼¼åº¦ + æ¶ˆæ­§ä¿¡æ¯è¿›è¡Œæ™ºèƒ½åˆ¤æ–­
        
        Args:
            e1: ç¬¬ä¸€ä¸ªå®ä½“
            e2: ç¬¬äºŒä¸ªå®ä½“
            entity_type: å®ä½“ç±»å‹
        
        Returns:
            Trueè¡¨ç¤ºåº”è¯¥åˆå¹¶ï¼ŒFalseè¡¨ç¤ºä¸åº”åˆå¹¶
        """
        # 1. è®¡ç®—æ ‡å‡†åŒ–åçš„åç§°ç›¸ä¼¼åº¦
        norm_name1 = self.normalize_name(e1['name'], entity_type)
        norm_name2 = self.normalize_name(e2['name'], entity_type)
        name_sim = self.calculate_similarity(norm_name1, norm_name2)
        
        # 2. è·å–æ¶ˆæ­§ä¿¡æ¯
        dis1 = e1.get('_disambiguator', '')
        dis2 = e2.get('_disambiguator', '')
        
        # âœ… è§„åˆ™1ï¼šåç§°é«˜åº¦ç›¸ä¼¼ + éƒ½æ— æ¶ˆæ­§ä¿¡æ¯ â†’ åˆå¹¶
        if name_sim >= 0.90 and not dis1 and not dis2:
            return True
        
        # âœ… è§„åˆ™2ï¼šåç§°é«˜åº¦ç›¸ä¼¼ + æ¶ˆæ­§ä¿¡æ¯ç›¸åŒ â†’ åˆå¹¶
        if name_sim >= 0.90 and dis1 and dis2 and dis1 == dis2:
            return True
        
        # âœ… è§„åˆ™3ï¼šåç§°å®Œå…¨ç›¸åŒ + æ¶ˆæ­§ä¿¡æ¯ç›¸ä¼¼ â†’ åˆå¹¶
        if name_sim >= 0.95 and dis1 and dis2:
            dis_sim = self.calculate_similarity(dis1, dis2)
            if dis_sim >= 0.80:
                return True
        
        # âŒ è§„åˆ™4ï¼šåç§°ç›¸ä¼¼ + æ¶ˆæ­§ä¿¡æ¯ä¸åŒ â†’ ä¸åˆå¹¶ï¼ˆä¿ç•™åŒåä¸åŒå®ä½“ï¼‰
        if name_sim >= 0.85 and dis1 and dis2 and dis1 != dis2:
            logger.info(
                f"[å»é‡] ä¿ç•™åŒåä¸åŒå®ä½“: '{e1['name']}' (æ¶ˆæ­§:{dis1}) "
                f"vs '{e2['name']}' (æ¶ˆæ­§:{dis2})"
            )
            return False
        
        # âŒ é»˜è®¤ï¼šåç§°ç›¸ä¼¼åº¦ä¸å¤Ÿ â†’ ä¸åˆå¹¶
        return False
    
    def _cluster_entities_smart(self, entities: List[Dict], entity_type: str) -> List[List[Dict]]:
        """
        âœ… æ™ºèƒ½èšç±»ï¼šåŒæ—¶è€ƒè™‘åç§°ç›¸ä¼¼åº¦ + æ¶ˆæ­§ä¿¡æ¯
        æ›¿ä»£åŸæ¥åªçœ‹åç§°ç›¸ä¼¼åº¦çš„ _cluster_entities æ–¹æ³•
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            entity_type: å®ä½“ç±»å‹
        
        Returns:
            èšç±»ç»“æœï¼Œæ¯ä¸ªç°‡æ˜¯ä¸€ä¸ªå®ä½“åˆ—è¡¨
        """
        visited = set()  # å·²è®¿é—®çš„å®ä½“ç´¢å¼•
        clusters = []
        
        for i, e1 in enumerate(entities):
            if i in visited:
                continue
            
            cluster = [e1]  # åˆ›å»ºæ–°ç°‡ï¼ŒåŒ…å«å½“å‰å®ä½“
            visited.add(i)
            
            # æŸ¥æ‰¾åº”è¯¥åˆå¹¶çš„å…¶ä»–å®ä½“
            for j, e2 in enumerate(entities):
                if j <= i or j in visited:  # é¿å…é‡å¤å¤„ç†
                    continue
                
                # âœ… ä½¿ç”¨å¤šç»´åº¦åˆ¤æ–­ï¼ˆåç§°ç›¸ä¼¼åº¦+æ¶ˆæ­§ä¿¡æ¯ï¼‰å†³å®šæ˜¯å¦åˆå¹¶
                if self._should_merge(e1, e2, entity_type):
                    cluster.append(e2)
                    visited.add(j)
            
            clusters.append(cluster)
        
        return clusters
    
    def _fuse_cluster(self, cluster: List[Dict], entity_type: str) -> Dict:
        """
        # TODO:è¯¥éƒ¨åˆ†å¯ä»¥äº¤ç»™å¤§æ¨¡å‹æŠ½å–
        èåˆä¸€ä¸ªç°‡çš„å®ä½“
        âœ… æ”¹è¿›ï¼šclusterç°åœ¨æ˜¯List[Dict]è€Œä¸æ˜¯List[Tuple]
        
        Args:
            cluster: å®ä½“ç°‡
            entity_type: å®ä½“ç±»å‹
        
        Returns:
            èåˆåçš„å®ä½“
        """
        # é€‰æ‹©æœ€é•¿çš„åç§°ä½œä¸ºæ ‡å‡†åï¼ˆé€šå¸¸æ›´å®Œæ•´ï¼‰
        canonical_name = max([e['name'] for e in cluster], key=len)
        
        # åˆå¹¶å±æ€§ - ç»Ÿè®¡å„å±æ€§å€¼çš„å‡ºç°æ¬¡æ•°
        merged_props = {}
        prop_counts = defaultdict(lambda: defaultdict(int))
        
        for entity in cluster:
            for prop, value in entity.get('props', {}).items():
                if value and prop != '_disambiguator':  # æ’é™¤å†…éƒ¨å­—æ®µ
                    prop_counts[prop][str(value)] += 1
        
        # é€‰æ‹©å‡ºç°æœ€å¤šçš„å€¼ä½œä¸ºæœ€ç»ˆå€¼
        for prop, value_counts in prop_counts.items():
            most_common = max(value_counts.items(), key=lambda x: x[1])
            merged_props[prop] = most_common[0]
        
        # ç”Ÿæˆå”¯ä¸€IDï¼ˆåŸºäºtype + nameï¼‰
        entity_id = hashlib.md5(f"{entity_type}_{canonical_name}".encode()).hexdigest()[:16]
        
        # âœ… è®°å½•åˆå¹¶æ¥æºæ•°é‡
        if len(cluster) > 1:
            logger.debug(f"[å»é‡] åˆå¹¶ {len(cluster)} ä¸ª '{canonical_name}' å®ä½“")
        
        return {
            'id': entity_id,  # å®ä½“å”¯ä¸€ID
            'type': entity_type,  # å®ä½“ç±»å‹
            'name': canonical_name,  # å®ä½“åç§°
            'props': merged_props,  # åˆå¹¶åçš„å±æ€§
            'source_count': len(cluster)  # æ¥æºå®ä½“æ•°é‡
        }


class RelationValidator:
    """å…³ç³»éªŒè¯å¼•æ“ - éªŒè¯å’Œè¿‡æ»¤å…³ç³»ï¼Œç¡®ä¿ç¬¦åˆSchemaçº¦æŸ"""
    
    def __init__(self, schema_edges: List[Dict]):
        """
        åˆå§‹åŒ–å…³ç³»éªŒè¯å™¨
        
        Args:
            schema_edges: Schemaä¸­å®šä¹‰çš„å…³ç³»è¾¹ï¼Œæ ¼å¼ä¸ºï¼š
                         [{'label': 'ä½äº', 'source': 'å…¬å¸', 'target': 'åŸå¸‚'}, ...]
        """
        # æ„å»ºå…³ç³»ç±»å‹åˆ°æº/ç›®æ ‡å®ä½“ç±»å‹çš„æ˜ å°„
        self.schema_map = {
            edge['label']: (edge['source'], edge['target'])
            for edge in schema_edges
        }

        # TODOï¼šå¾…ä¿®æ”¹
        # å•å€¼å…³ç³»ï¼ˆåŒä¸€subjectåªèƒ½æœ‰ä¸€ä¸ªobjectï¼‰- è¿™äº›å…³ç³»åœ¨ç°å®ä¸­é€šå¸¸æ˜¯å”¯ä¸€çš„
        self.single_value_relations = {'å‡ºç”Ÿäº', 'æˆç«‹äº', 'æ€»éƒ¨ä½äº', 'æ¯•ä¸šäº'}
    
    def validate(self, relations: List[Dict], entity_mapping: Dict) -> List[Dict]:
        """
        å…³ç³»éªŒè¯å’Œå»é‡
        
        Args:
            relations: åŸå§‹å…³ç³»åˆ—è¡¨ï¼Œæ ¼å¼ä¸ºï¼š
                      [{'type': 'ä½äº', 'subject': {...}, 'object': {...}}, ...]
            entity_mapping: å®ä½“æ˜ å°„å­—å…¸ï¼Œæ ¼å¼ä¸º{(type, name): canonical_id}
        
        Returns:
            éªŒè¯åçš„å…³ç³»åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹å…³ç³»éªŒè¯ï¼ŒåŸå§‹å…³ç³»æ•°: {len(relations)}")
        
        # 1. SchemaéªŒè¯ - æ£€æŸ¥å…³ç³»æ˜¯å¦ç¬¦åˆé¢„å®šä¹‰çš„Schema
        valid = []
        for rel in relations:
            if not self._check_schema(rel):
                continue
            valid.append(rel)
        
        logger.info(f"  SchemaéªŒè¯: {len(relations)} -> {len(valid)}")
        
        # 2. å®ä½“æ˜ å°„ - å°†åŸå§‹å®ä½“æ˜ å°„åˆ°æ ‡å‡†åŒ–å®ä½“ID
        mapped = []
        for rel in valid:
            subject = rel.get('subject') or {}
            obj = rel.get('object') or {}

            subj_type = subject.get('type')
            obj_type = obj.get('type')
            subj_name = subject.get('name')
            obj_name = obj.get('name')

            if not subj_type or not obj_type or not subj_name or not obj_name:
                logger.warning(f"å…³ç³»ç¼ºå°‘å®ä½“ä¿¡æ¯ï¼Œè·³è¿‡: {rel}")
                continue

            # æ„å»ºå®ä½“æ˜ å°„é”®
            subj_key = (subj_type, subj_name)
            obj_key = (obj_type, obj_name)
            
            # è·å–æ ‡å‡†åŒ–å®ä½“ID
            subj_id = entity_mapping.get(subj_key)
            obj_id = entity_mapping.get(obj_key)
            
            if not subj_id or not obj_id:
                logger.warning(f"å®ä½“æ˜ å°„å¤±è´¥: {subj_key} -> {obj_key}")
                continue
            
            # åˆ›å»ºæ˜ å°„åçš„å…³ç³»
            mapped.append({
                'type': rel.get('type'),
                'subject_id': subj_id,
                'object_id': obj_id
            })
        
        logger.info(f"  å®ä½“æ˜ å°„: {len(valid)} -> {len(mapped)}")
        
        # 3. å»é‡ - åŸºäº(type, subject_id, object_id)è¿›è¡Œå»é‡
        unique = self._deduplicate(mapped)
        logger.info(f"  å»é‡: {len(mapped)} -> {len(unique)}")
        
        # 4. çº¦æŸæ£€æŸ¥ - åº”ç”¨ä¸šåŠ¡è§„åˆ™
        final = self._apply_constraints(unique)
        logger.info(f"  çº¦æŸæ£€æŸ¥: {len(unique)} -> {len(final)}")
        
        return final
    
    def _check_schema(self, relation: Dict) -> bool:
        """
        # å¾…ä¿®æ”¹
        æ£€æŸ¥å…³ç³»æ˜¯å¦ç¬¦åˆschemaå®šä¹‰
        
        Args:
            relation: å¾…éªŒè¯çš„å…³ç³»
        
        Returns:
            Trueè¡¨ç¤ºç¬¦åˆSchemaï¼ŒFalseè¡¨ç¤ºä¸ç¬¦åˆ
        """
        rel_type = relation.get('type')
        if not rel_type:
            logger.warning(f"[SchemaéªŒè¯] å…³ç³»ç¼ºå°‘ç±»å‹ï¼Œè·³è¿‡: {relation}")
            return False
        
        # æ£€æŸ¥å…³ç³»ç±»å‹æ˜¯å¦åœ¨Schemaä¸­å®šä¹‰
        if rel_type not in self.schema_map:
            logger.warning(f"[SchemaéªŒè¯] æœªå®šä¹‰çš„å…³ç³»ç±»å‹: '{rel_type}', å¯ç”¨ç±»å‹: {list(self.schema_map.keys())}")
            logger.warning(f"[SchemaéªŒè¯] è¢«æ‹’ç»çš„å…³ç³»è¯¦æƒ…: {relation}")
            return False
        
        # è·å–Schemaä¸­å®šä¹‰çš„æº/ç›®æ ‡å®ä½“ç±»å‹
        expected_source, expected_target = self.schema_map[rel_type]
        subject = relation.get('subject') or {}
        obj = relation.get('object') or {}
        # è·å–å®é™…çš„å®ä½“ç±»å‹
        actual_source = subject.get('type') or subject.get('label')
        actual_target = obj.get('type') or obj.get('label')
        
        if not actual_source or not actual_target:
            logger.warning(f"[SchemaéªŒè¯] å…³ç³»ç¼ºå°‘å®ä½“ç±»å‹ä¿¡æ¯ï¼Œè·³è¿‡: {relation}")
            logger.warning(f"[SchemaéªŒè¯] subject: {subject}, object: {obj}")
            return False
        
        # æ£€æŸ¥å®é™…å®ä½“ç±»å‹æ˜¯å¦ä¸Schemaå®šä¹‰åŒ¹é…
        if actual_source != expected_source or actual_target != expected_target:
            logger.warning(f"[SchemaéªŒè¯] å®ä½“ç±»å‹ä¸åŒ¹é…:")
            logger.warning(f"  å…³ç³»ç±»å‹: '{rel_type}'")
            logger.warning(f"  æœŸæœ›: {expected_source} -> {expected_target}")
            logger.warning(f"  å®é™…: {actual_source} -> {actual_target}")
            logger.warning(f"  å®Œæ•´å…³ç³»: {relation}")
            return False
        
        return True
    
    def _deduplicate(self, relations: List[Dict]) -> List[Dict]:
        """
        å…³ç³»å»é‡ - åŸºäº(type, subject_id, object_id)è¿›è¡Œå»é‡
        
        Args:
            relations: å…³ç³»åˆ—è¡¨
        
        Returns:
            å»é‡åçš„å…³ç³»åˆ—è¡¨
        """
        unique_dict = {}  # ç”¨äºå­˜å‚¨å”¯ä¸€å…³ç³»
        counts = defaultdict(int)  # ç»Ÿè®¡æ¯ä¸ªå…³ç³»çš„å‡ºç°æ¬¡æ•°
        
        for rel in relations:
            key = (rel['type'], rel['subject_id'], rel['object_id'])  # æ„å»ºå”¯ä¸€é”®
            counts[key] += 1
            unique_dict[key] = rel  # ä¿ç•™æœ€åå‡ºç°çš„å…³ç³»
        
        # æ·»åŠ å‡ºç°æ¬¡æ•°åˆ°å…³ç³»ä¸­
        for key, rel in unique_dict.items():
            rel['count'] = counts[key]
        
        return list(unique_dict.values())
    
    def _apply_constraints(self, relations: List[Dict]) -> List[Dict]:
        """
        åº”ç”¨çº¦æŸè§„åˆ™ - å¤„ç†å•å€¼å…³ç³»ç­‰çº¦æŸï¼Œå•å€¼å…³ç³»æ˜¯æŒ‡ä¸€ä¸ªä¸»ä½“åªèƒ½æœ‰ä¸€ä¸ªè¿™æ ·çš„å…³ç³»
        
        Args:
            relations: å…³ç³»åˆ—è¡¨
        
        Returns:
            åº”ç”¨çº¦æŸåçš„å…³ç³»åˆ—è¡¨
        """
        # æŒ‰(subject_id, type)åˆ†ç»„ - ä¾¿äºå¤„ç†å•å€¼å…³ç³»çº¦æŸ
        by_subject = defaultdict(list)
        for rel in relations:
            by_subject[(rel['subject_id'], rel['type'])].append(rel)
        
        final = []
        for (subj_id, rel_type), rels in by_subject.items():
            if rel_type in self.single_value_relations:
                # å•å€¼å…³ç³»ï¼šä¿ç•™å‡ºç°æ¬¡æ•°æœ€å¤šçš„
                best = max(rels, key=lambda r: r.get('count', 0))
                final.append(best)
                if len(rels) > 1:
                    logger.info(f"å•å€¼å…³ç³»å†²çª '{rel_type}': ä¿ç•™1ä¸ªï¼Œä¸¢å¼ƒ{len(rels)-1}ä¸ª")
            else:
                # éå•å€¼å…³ç³»ï¼šä¿ç•™æ‰€æœ‰å…³ç³»
                final.extend(rels)
        
        return final


class ExtractServiceOptimized:
    """ä¼˜åŒ–åçš„æŠ½å–æœåŠ¡ - æä¾›å®Œæ•´çš„çŸ¥è¯†æŠ½å–æµç¨‹"""
    
    @staticmethod
    async def extract_from_mapping_task_optimized(mapping_id: str, graph_name: str):
        """
        âœ… å®Œæ•´ä¼˜åŒ–çš„æŠ½å–æµç¨‹
        
        æ”¹è¿›:
        1. ä¸€æ¬¡æ€§æŠ½å–ï¼ˆå®ä½“+å…³ç³»ï¼‰
        2. å¹¶å‘æ‰§è¡Œï¼ˆSemaphore=20ï¼‰
        3. å®ä½“å»é‡
        4. å…³ç³»éªŒè¯
        5. æ‰¹é‡å¯¼å…¥
        
        Args:
            mapping_id: æ˜ å°„IDï¼Œå®šä¹‰äº†æŠ½å–ä»»åŠ¡çš„é…ç½®
            graph_name: å›¾è°±åç§°ï¼ŒæŒ‡å®šæŠ½å–ç»“æœå­˜å‚¨çš„å›¾è°±
        
        Returns:
            æŠ½å–ç»“æœç»Ÿè®¡ä¿¡æ¯
        """
        start_time = asyncio.get_event_loop().time()  # è®°å½•å¼€å§‹æ—¶é—´
        
        # è·å–æŠ½å–é…ç½®ä¿¡æ¯
        mapping = (await mapping_service.get_mapping_by_id(mapping_id)).model_dump()
        data_collection_id = mapping['data_collection_id']  # æ•°æ®é›†ID
        schema_id = mapping['schema_id']  # Schema ID
        model_id = mapping.get('model_id')  # æ¨¡å‹ID
        
        # è·å–æ–‡æ¡£åˆ—è¡¨
        document_ids = await asyncio.to_thread(
            DataServiceHandler.get_parsed_documents,
            [data_collection_id],
            "langchain"  # æŒ‡å®šæ–‡æ¡£è§£ææ–¹å¼
        )
        logger.info(f"[ä¼˜åŒ–æŠ½å–] æ–‡æ¡£æ•°: {len(document_ids)}")
        
        # è·å–Schemaå®šä¹‰
        schema = (await schema_service.get_schema_by_id(schema_id)).model_dump()
        
        # æå–å®ä½“å’Œå…³ç³»çš„Schemaå®šä¹‰
        entity_schema = await ExtractServiceOptimized._extract_entity_schema(schema)
        relation_schema = await ExtractServiceOptimized._extract_relation_schema(schema)
        
        # æ„å»ºIDåˆ°labelçš„æ˜ å°„ï¼Œå¹¶è½¬æ¢edges
        nodes_map = {n['id']: n['label'] for n in schema.get('schema_graph', {}).get('nodes', [])}
        schema_edges = []
        for edge in schema.get('schema_graph', {}).get('edges', []):
            schema_edges.append({
                'label': edge.get('label', ''),
                'source': nodes_map.get(edge.get('source', ''), ''),  # ID -> Label
                'target': nodes_map.get(edge.get('target', ''), '')   # ID -> Label
            })
        
        logger.info(f"[ä¼˜åŒ–æŠ½å–] Schema: {len(entity_schema)}ä¸ªå®ä½“ç±»å‹, {len(relation_schema)}ä¸ªå…³ç³»ç±»å‹")
        
        # ==== é˜¶æ®µ1: å¹¶å‘æŠ½å– ====
        logger.info("[ä¼˜åŒ–æŠ½å–] é˜¶æ®µ1: å¹¶å‘ä¸€æ¬¡æ€§æŠ½å–")
        raw_entities, raw_relations = await ExtractServiceOptimized._parallel_extract_once(
            document_ids, entity_schema, relation_schema, model_id
        )
        raw_entities = ExtractServiceOptimized._filter_invalid_entities(raw_entities)
        raw_relations = ExtractServiceOptimized._filter_invalid_relations(raw_relations)
        
        extract_time = asyncio.get_event_loop().time() - start_time
        logger.info(f"[ä¼˜åŒ–æŠ½å–] æŠ½å–å®Œæˆ: {len(raw_entities)}å®ä½“, {len(raw_relations)}å…³ç³», è€—æ—¶{extract_time:.1f}s")
        
        # ==== é˜¶æ®µ2: å®ä½“å»é‡ ====
        logger.info("[ä¼˜åŒ–æŠ½å–] é˜¶æ®µ2: å®ä½“å»é‡")
        dedup_start = asyncio.get_event_loop().time()
        
        entity_deduplicator = EntityDeduplicator()
        unique_entities, entity_mapping = entity_deduplicator.deduplicate(raw_entities)
        
        dedup_time = asyncio.get_event_loop().time() - dedup_start
        logger.info(f"[ä¼˜åŒ–æŠ½å–] å»é‡å®Œæˆ: {len(raw_entities)} -> {len(unique_entities)}ä¸ªå”¯ä¸€å®ä½“, è€—æ—¶: {dedup_time:.2f}ç§’")
        
        # ==== é˜¶æ®µ3: å…³ç³»éªŒè¯ ====
        logger.info("[ä¼˜åŒ–æŠ½å–] é˜¶æ®µ3: å…³ç³»éªŒè¯")
        validate_start = asyncio.get_event_loop().time()
        
        relation_validator = RelationValidator(schema_edges)
        valid_relations = relation_validator.validate(raw_relations, entity_mapping)
        
        validate_time = asyncio.get_event_loop().time() - validate_start
        logger.info(f"[ä¼˜åŒ–æŠ½å–] éªŒè¯å®Œæˆ: è€—æ—¶: {validate_time:.2f}ç§’")
        
        # ==== é˜¶æ®µ4: è½¬æ¢ä¸ºTuGraphæ ¼å¼ ====
        logger.info("[ä¼˜åŒ–æŠ½å–] é˜¶æ®µ4: æ ¼å¼è½¬æ¢")
        tugraph_nodes = await ExtractServiceOptimized._convert_to_tugraph_nodes(unique_entities, schema)
        tugraph_relations = await ExtractServiceOptimized._convert_to_tugraph_relations(valid_relations, unique_entities)
        
        # âœ… è°ƒè¯•æ—¥å¿—ï¼šè®°å½•è½¬æ¢åçš„èŠ‚ç‚¹ç¤ºä¾‹
        logger.info(f"[TuGraphè°ƒè¯•] è½¬æ¢åèŠ‚ç‚¹æ•°é‡: {len(tugraph_nodes)}")
        for idx, node in enumerate(tugraph_nodes[:3]):  # åªè®°å½•å‰3ä¸ª
            logger.info(f"[TuGraphè°ƒè¯•] è½¬æ¢åèŠ‚ç‚¹{idx+1}: label={node.get('label')}, name={node.get('name')}, props={node.get('props')}")
        
        # âœ… ç»Ÿè®¡å…³ç³»ä¸­ä½¿ç”¨çš„èŠ‚ç‚¹
        nodes_in_relations = set()
        for rel in tugraph_relations:
            nodes_in_relations.add(rel['start'])
            nodes_in_relations.add(rel['end'])
        
        isolated_nodes = len(tugraph_nodes) - len(nodes_in_relations)
        if isolated_nodes > 0:
            logger.warning(f"âš ï¸ å‘ç°{isolated_nodes}ä¸ªå­¤ç«‹èŠ‚ç‚¹ï¼ˆæ— å…³ç³»æŒ‡å‘ï¼‰ï¼Œtotal={len(tugraph_nodes)}, connected={len(nodes_in_relations)}")
        
        # ==== é˜¶æ®µ5: æ‰¹é‡å¯¼å…¥ ====
        logger.info("[ä¼˜åŒ–æŠ½å–] é˜¶æ®µ5: æ‰¹é‡å¯¼å…¥")
        import_start = asyncio.get_event_loop().time()
        
        await ExtractServiceOptimized._batch_import(tugraph_nodes, tugraph_relations, graph_name)
        
        import_time = asyncio.get_event_loop().time() - import_start
        total_time = asyncio.get_event_loop().time() - start_time
        
        logger.info(f"[ä¼˜åŒ–æŠ½å–] å®Œæˆ! æ€»è€—æ—¶: {total_time:.2f}ç§’ (æŠ½å–: {extract_time:.2f}s, å»é‡: {dedup_time:.2f}s, éªŒè¯: {validate_time:.2f}s, å¯¼å…¥: {import_time:.2f}s)")
        logger.info(f"[ä¼˜åŒ–æŠ½å–] æœ€ç»ˆç»“æœ: {len(tugraph_nodes)}ä¸ªèŠ‚ç‚¹, {len(tugraph_relations)}ä¸ªå…³ç³»")
        
        return {
            'nodes': len(tugraph_nodes),  # èŠ‚ç‚¹æ•°é‡
            'relations': len(tugraph_relations),  # å…³ç³»æ•°é‡
            'time_seconds': total_time  # æ€»è€—æ—¶
        }
    
    @staticmethod
    async def _parallel_extract_once(document_ids: List[str], entity_schema: List, relation_schema: List, model_id: str) -> Tuple[List[Dict], List[Dict]]:
        """
        âœ… ä¼˜åŒ–ï¼šæ™ºèƒ½å¹¶å‘æ§åˆ¶ï¼Œæ ¹æ®ä»»åŠ¡è§„æ¨¡åŠ¨æ€è°ƒæ•´
        - å°ä»»åŠ¡ï¼ˆ<20æ®µï¼‰ï¼š10å¹¶å‘ï¼Œæ—¥å¸¸ä½¿ç”¨
        - ä¸­ç­‰ä»»åŠ¡ï¼ˆ20-100æ®µï¼‰ï¼š20å¹¶å‘ï¼Œå¹³è¡¡æ€§èƒ½
        - å¤§ä»»åŠ¡ï¼ˆ>100æ®µï¼‰ï¼š40å¹¶å‘ï¼Œé«˜åå
        
        Args:
            document_ids: æ–‡æ¡£IDåˆ—è¡¨
            entity_schema: å®ä½“Schemaå®šä¹‰
            relation_schema: å…³ç³»Schemaå®šä¹‰
            model_id: æ¨¡å‹ID
        
        Returns:
            (entities, relations) - å®ä½“åˆ—è¡¨å’Œå…³ç³»åˆ—è¡¨
        """
        from app.utils.model_util import call_model_to_extract_combined  # å¯¼å…¥æ¨¡å‹è°ƒç”¨å‡½æ•°
        
        # âœ… æ˜¾ç¤ºç¼“å­˜çŠ¶æ€
        cache_stats = extraction_cache.get_stats()
        if cache_stats.get("enabled"):
            logger.info(f"[ç¼“å­˜] å·²å¯ç”¨ï¼Œå½“å‰ç¼“å­˜: {cache_stats['count']}ä¸ªæ–‡ä»¶, {cache_stats['total_size_mb']}MB")
        
        tasks = []
        # âœ… æ™ºèƒ½å¹¶å‘ï¼šæ ¹æ®ä»»åŠ¡è§„æ¨¡è‡ªé€‚åº”è°ƒæ•´
        # TODOï¼šæ ¹æ®æ–‡æ¡£å¤§å°æ§åˆ¶å¹¶å‘æ•°
        task_count_estimate = len(document_ids) * 2  # ç²—ç•¥ä¼°è®¡æ®µè½æ•°
        if task_count_estimate <= 20:
            concurrency = 10  # æ—¥å¸¸ä»»åŠ¡ï¼š10å¹¶å‘
        elif task_count_estimate <= 100:
            concurrency = 20  # ä¸­ç­‰ä»»åŠ¡ï¼š20å¹¶å‘
        else:
            concurrency = 40  # å¤§æ‰¹é‡ï¼š40å¹¶å‘
        
        # åˆ›å»ºå¹¶å‘æ§åˆ¶ä¿¡å·é‡
        semaphore = asyncio.Semaphore(concurrency)
        logger.info(f"[å¹¶å‘æŠ½å–] æ–‡æ¡£æ•°: {len(document_ids)}, é¢„ä¼°æ®µè½: {task_count_estimate}, å¹¶å‘åº¦: {concurrency}")

        async def run_extract(content: str):
            """
            æ‰§è¡Œå•æ¬¡æŠ½å–çš„å¼‚æ­¥å‡½æ•°
            
            Args:
                content: æ–‡æœ¬å†…å®¹
            
            Returns:
                æŠ½å–ç»“æœ
            """
            async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°
                # âœ… ä¼˜å…ˆä½¿ç”¨ç¼“å­˜
                cached_result = extraction_cache.get(content, entity_schema, relation_schema)
                if cached_result is not None:
                    return cached_result
                
                # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨LLMè¿›è¡ŒæŠ½å–
                result = await call_model_to_extract_combined(
                    content,
                    entity_schema,
                    relation_schema,
                    model_id
                )
                
                # âœ… ä¿å­˜åˆ°ç¼“å­˜
                if result:
                    extraction_cache.set(content, entity_schema, relation_schema, result)
                
                return result
        
        # ä¸ºæ¯ä¸ªæ–‡æ¡£çš„æ¯ä¸ªæ®µè½åˆ›å»ºæŠ½å–ä»»åŠ¡
        for doc_id in document_ids:
            try:
                document = await asyncio.to_thread(DataServiceHandler.get_document_detail, doc_id)
            except Exception as e:
                logger.error(f"[parallel_extract] fetch document failed, id={doc_id}, err: {e}")
                continue
            if not document:
                continue
            
            segments = document.get("segments") or []
            for segment in segments:
                content = segment.get("content")
                if not content or not str(content).strip():
                    continue
                tasks.append(run_extract(str(content)))

        is_small_task = len(tasks) <= 10
        if is_small_task:
            logger.info(f"[parallel_extract] è½»é‡ä»»åŠ¡: {len(tasks)}æ®µï¼Œå¹¶å‘{concurrency}")
        else:
            logger.info(f"[parallel_extract] total tasks: {len(tasks)}")
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰æŠ½å–ä»»åŠ¡
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # åˆ†ç¦»å®ä½“å’Œå…³ç³»
        all_entities = []
        all_relations = []
        
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"[parallel_extract] task failed: {result}")
                continue
            
            if not result:
                continue
            
            if not isinstance(result, dict):
                logger.warning(f"[parallel_extract] unexpected result type, skip: {result}")
                continue
            
            # æå–å®ä½“
            entities = result.get('entities', {})
            for entity_type, entity_dict in entities.items():
                for entity_name, props in entity_dict.items():
                    all_entities.append({
                        'type': entity_type,
                        'name': entity_name,
                        'props': props
                    })
            
            # æå–å…³ç³»
            relations = result.get('relations', {})
            for rel_type, rel_list in relations.items():
                for rel in rel_list:
                    all_relations.append({
                        'type': rel_type,
                        'subject': rel.get('subject', {}),
                        'object': rel.get('object', {})
                    })
        
        # âœ… æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡
        cache_stats = extraction_cache.get_stats()
        if cache_stats.get("enabled") and (cache_stats["hit_count"] + cache_stats["miss_count"]) > 0:
            logger.info(f"[ç¼“å­˜] å‘½ä¸­ç‡: {cache_stats['hit_rate']} (å‘½ä¸­{cache_stats['hit_count']}/æ€»è®¡{cache_stats['hit_count'] + cache_stats['miss_count']})")
        
        return all_entities, all_relations
    
    @staticmethod
    def _filter_invalid_entities(entities: List[Dict]) -> List[Dict]:
        """
        è¿‡æ»¤æ— æ•ˆå®ä½“ - ç§»é™¤ç¼ºå°‘å¿…è¦å­—æ®µçš„å®ä½“å¹¶æ ‡å‡†åŒ–å±æ€§
        
        Args:
            entities: å®ä½“åˆ—è¡¨
        
        Returns:
            è¿‡æ»¤åçš„å®ä½“åˆ—è¡¨
        """
        cleaned = []
        dropped = 0
        
        for entity in entities:
            entity_type = entity.get('type')
            name = entity.get('name')
            if not entity_type or not name:
                dropped += 1
                logger.warning(f"[extract] drop invalid entity: {entity}")
                continue
            
            cleaned.append({
                'type': entity_type,
                'name': name,
                'props': entity.get('props') or {}  # ç¡®ä¿propsæ˜¯å­—å…¸
            })
        
        if dropped:
            logger.info(f"[extract] filtered invalid entities: {dropped}")
        return cleaned
    
    @staticmethod
    def _filter_invalid_relations(relations: List[Dict]) -> List[Dict]:
        """
        è¿‡æ»¤æ— æ•ˆå…³ç³» - ç§»é™¤ç¼ºå°‘ä¸»ä½“/å®¢ä½“/ç±»å‹çš„å…³ç³»å¹¶ç»Ÿä¸€å­—æ®µ
        
        Args:
            relations: å…³ç³»åˆ—è¡¨
        
        Returns:
            è¿‡æ»¤åçš„å…³ç³»åˆ—è¡¨
        """
        cleaned = []
        dropped = 0
        
        for rel in relations:
            rel_type = rel.get('type')
            subject = rel.get('subject') or {}
            obj = rel.get('object') or {}
            
            # è·å–å®ä½“ç±»å‹ï¼ˆå¯èƒ½æ˜¯typeæˆ–labelå­—æ®µï¼‰
            subj_type = subject.get('type') or subject.get('label')
            obj_type = obj.get('type') or obj.get('label')
            subj_name = subject.get('name')
            obj_name = obj.get('name')
            
            # æ ‡å‡†åŒ–å®ä½“ç±»å‹å­—æ®µ
            if subj_type:
                subject = {**subject, 'type': subj_type}
            if obj_type:
                obj = {**obj, 'type': obj_type}
            
            # æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
            if not rel_type or not subj_type or not obj_type or not subj_name or not obj_name:
                dropped += 1
                logger.warning(f"[extract] drop invalid relation: {rel}")
                continue
            
            cleaned.append({
                'type': rel_type,
                'subject': subject,
                'object': obj
            })
        
        if dropped:
            logger.info(f"[extract] filtered invalid relations: {dropped}")
        return cleaned
    
    @staticmethod
    async def _extract_entity_schema(schema: Dict) -> List[Dict]:
        """
        âœ… æå–å®ä½“Schemaå®šä¹‰ï¼ŒåŒ…å«è¯¦ç»†çš„å±æ€§ä¿¡æ¯
        
        Args:
            schema: å®Œæ•´çš„Schemaå®šä¹‰
        
        Returns:
            å®ä½“Schemaåˆ—è¡¨
        """
        entities = []
        for node in schema.get('schema_graph', {}).get('nodes', []):
            # âœ… æå–è¯¦ç»†çš„å±æ€§ä¿¡æ¯ï¼ˆåç§°ã€ç±»å‹ã€æè¿°ï¼‰
            attributes = []
            
            # âœ… å…³é”®ä¿®å¤ï¼šSchemaä¸­å­—æ®µåæ˜¯ 'attr' ä¸æ˜¯ 'attributes'
            for attr in node.get('attr', []):  # â† ä¿®å¤ï¼šattr
                # âœ… Schemaä¸­å±æ€§åå­—æ®µæ˜¯ 'label' ä¸æ˜¯ 'name'
                attr_name = attr.get('label', '')  # â† ä¿®å¤ï¼šlabel
                attr_type = attr.get('type', 'STRING')  # â† ä¿®å¤ï¼štypeï¼ˆä¸æ˜¯dataTypeï¼‰
                attr_desc = attr.get('description', '')  # å±æ€§æè¿°
                
                # âš ï¸ è¿‡æ»¤æ‰ç³»ç»Ÿå­—æ®µï¼ˆnodeIdã€nodeNameï¼‰
                if attr_name in ['nodeId', 'nodeName']:
                    continue
                
                if attr_name:  # åªæ·»åŠ æœ‰åç§°çš„å±æ€§
                    # æ„å»ºå±æ€§å­—ç¬¦ä¸²ï¼šåç§° (ç±»å‹): æè¿°
                    if attr_desc:
                        attr_str = f"{attr_name} ({attr_type}): {attr_desc}"
                    else:
                        attr_str = f"{attr_name} ({attr_type})"
                    attributes.append(attr_str)
            
            entity_info = {
                'entity_type': node.get('label', ''),  # å®ä½“ç±»å‹åç§°
                'attributes': attributes,  # âœ… è¯¦ç»†å±æ€§åˆ—è¡¨
                'description': node.get('description', '')  # å®ä½“ç±»å‹æè¿°
            }
            entities.append(entity_info)
        return entities
    
    @staticmethod
    async def _extract_relation_schema(schema: Dict) -> List[Dict]:
        """
        æå–å…³ç³»Schemaå®šä¹‰
        
        Args:
            schema: å®Œæ•´çš„Schemaå®šä¹‰
        
        Returns:
            å…³ç³»Schemaåˆ—è¡¨
        """
        relations = []
        # æ„å»ºèŠ‚ç‚¹IDåˆ°æ ‡ç­¾çš„æ˜ å°„
        nodes_map = {n['id']: n['label'] for n in schema.get('schema_graph', {}).get('nodes', [])}
        
        for edge in schema.get('schema_graph', {}).get('edges', []):
            relation_info = {
                'relation_type': edge.get('label', ''),  # å…³ç³»ç±»å‹åç§°
                'source_type': nodes_map.get(edge.get('source', ''), ''),  # æºå®ä½“ç±»å‹
                'target_type': nodes_map.get(edge.get('target'), '')  # ç›®æ ‡å®ä½“ç±»å‹
            }
            relations.append(relation_info)
        return relations
    
    @staticmethod
    async def _convert_to_tugraph_nodes(entities: List[Dict], schema: Dict) -> List[Dict]:
        """
        âœ… è½¬æ¢å®ä½“ä¸ºTuGraphèŠ‚ç‚¹æ ¼å¼ï¼ˆä¸¥æ ¼éµå¾ªæ—§ç‰ˆæ ¼å¼ï¼‰
        
        Args:
            entities: å®ä½“åˆ—è¡¨
            schema: Schemaå®šä¹‰
        
        Returns:
            TuGraphèŠ‚ç‚¹åˆ—è¡¨
        """
        # âœ… æ„å»ºæ¯ä¸ªèŠ‚ç‚¹ç±»å‹çš„åˆæ³•å­—æ®µæ˜ å°„ï¼ˆä»Schemaä¸­æå–ï¼‰
        allowed_fields_map = {}
        base_fields = {"name", "nodeName", "nodeId"}  # åŸºç¡€å­—æ®µæ€»æ˜¯å…è®¸çš„
        
        # éå†Schemaä¸­çš„èŠ‚ç‚¹å®šä¹‰ï¼Œæå–å…è®¸çš„å­—æ®µ
        for node in schema.get('schema_graph', {}).get('nodes', []):
            node_label = node.get('label', '')
            if not node_label:
                continue
            
            # æ”¶é›†è¯¥èŠ‚ç‚¹ç±»å‹åœ¨Schemaä¸­å®šä¹‰çš„æ‰€æœ‰å±æ€§
            allowed_fields = set(base_fields)  # å¤åˆ¶åŸºç¡€å­—æ®µ
            for attr in node.get('attr', []):
                attr_name = attr.get('label')
                if attr_name:
                    allowed_fields.add(attr_name)
            
            allowed_fields_map[node_label] = allowed_fields
        
        nodes = []
        for entity in entities:
            entity_type = entity['type']
            entity_id = entity.get('id', entity['name'])
            
            # è·å–è¯¥å®ä½“ç±»å‹çš„åˆæ³•å­—æ®µé›†åˆ
            allowed_fields = allowed_fields_map.get(entity_type, set(base_fields))
            
            # âœ… æ™ºèƒ½è¿‡æ»¤ï¼šåªä¿ç•™Schemaä¸­å®šä¹‰çš„å­—æ®µ
            raw_props = entity.get('props', {})
            filtered_props = {}
            dropped_fields = []
            
            for key, value in raw_props.items():
                if key in allowed_fields:
                    filtered_props[key] = value
                else:
                    # å­—æ®µæœªåœ¨Schemaä¸­å®šä¹‰ï¼Œä¸¢å¼ƒï¼ˆé¿å…TuGraphæŠ¥é”™ï¼‰
                    dropped_fields.append(key)
            
            # åªåœ¨æœ‰è¾ƒå¤šå­—æ®µè¢«ä¸¢å¼ƒæ—¶è®°å½•è­¦å‘Š
            if len(dropped_fields) > 2:
                logger.warning(f"[å­—æ®µè¿‡æ»¤] '{entity['name']}'({entity_type}): ä¸¢å¼ƒ{len(dropped_fields)}ä¸ªæœªå®šä¹‰å­—æ®µ")
            
            # nodeIdæ˜¯å¿…éœ€å­—æ®µï¼Œæ€»æ˜¯æ·»åŠ 
            filtered_props['nodeId'] = entity_id
            
            # âŒ ä¸è¦æ·»åŠ nodeNameï¼kg_serviceä¼šè‡ªåŠ¨ä»node.nameç”Ÿæˆ
            
            node = {
                'label': entity_type,  # èŠ‚ç‚¹æ ‡ç­¾
                'name': entity['name'],  # èŠ‚ç‚¹åç§°
                'props': filtered_props  # âœ… åªåŒ…å«Schemaå®šä¹‰çš„åˆæ³•å­—æ®µ
            }
            nodes.append(node)
        
        return nodes
    
    @staticmethod
    async def _convert_to_tugraph_relations(relations: List[Dict], entities: List[Dict]) -> List[Dict]:
        """
        âœ… è½¬æ¢å…³ç³»ä¸ºTuGraphè¾¹æ ¼å¼ï¼ˆä¸æ—§ç‰ˆæ ¼å¼ä¸€è‡´ï¼‰
        
        Args:
            relations: å…³ç³»åˆ—è¡¨
            entities: å®ä½“åˆ—è¡¨
        
        Returns:
            TuGraphå…³ç³»åˆ—è¡¨
        """
        # æ„å»ºIDåˆ°å®ä½“çš„æ˜ å°„
        id_to_entity = {e['id']: e for e in entities}
        
        edges = []
        for rel in relations:
            subj_entity = id_to_entity.get(rel['subject_id'])
            obj_entity = id_to_entity.get(rel['object_id'])
            
            if not subj_entity or not obj_entity:
                continue
            
            # âœ… ä¿®å¤ï¼šä½¿ç”¨æ—§ç‰ˆæ ¼å¼ start/end/type
            # æ—§ç‰ˆä¸­ä½¿ç”¨å®Œæ•´IDï¼Œä½†éç»“æ„åŒ–æŠ½å–ä¸IDï¼Œä½¿ç”¨nameä»£æ›¿
            edge = {
                'start': subj_entity.get('id') or subj_entity['name'],  # âœ… start not start_node
                'end': obj_entity.get('id') or obj_entity['name'],      # âœ… end not end_node
                'type': rel['type']                                     # âœ… type not label
            }
            edges.append(edge)
        
        return edges
    
    @staticmethod
    async def _batch_import(nodes: List[Dict], relations: List[Dict], graph_name: str, batch_size: int = 100):
        """
        âœ… æ··åˆæ¨¡å¼ï¼šä¼˜å…ˆæ‰¹é‡å¯¼å…¥ï¼ˆå¿«ï¼‰ï¼Œå¤±è´¥åˆ™é€ä¸ªå¯¼å…¥ï¼ˆç¨³ï¼‰
        - æ‰¹é‡å¯¼å…¥ï¼šæ€§èƒ½æå‡14å€ï¼ˆ2900èŠ‚ç‚¹ï¼š290ç§’ -> 20ç§’ï¼‰
        - é™çº§é€ä¸ªï¼šä¿è¯ç¨³å®šæ€§ï¼ˆå…¼å®¹å±æ€§ç¼ºå¤±ï¼‰
        
        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            relations: å…³ç³»åˆ—è¡¨
            graph_name: å›¾è°±åç§°
            batch_size: æ‰¹æ¬¡å¤§å°
        """
        logger.info(f"[batch_import] start, nodes={len(nodes)}, relations={len(relations)}")
        
        # ============ èŠ‚ç‚¹å¯¼å…¥ï¼šæ¸è¿›å¼é™çº§ ============
        nodes_imported = 0
        total_nodes = len(nodes)
        
        if total_nodes > 0:
            # ğŸš€ ç­–ç•¥1ï¼šä¼˜å…ˆå°è¯•å…¨éƒ¨æ‰¹é‡å¯¼å…¥ï¼ˆæœ€å¿«ï¼‰
            logger.info(f"[batch_import] å°è¯•æ‰¹é‡å¯¼å…¥{total_nodes}ä¸ªèŠ‚ç‚¹...")
            try:
                import_start = asyncio.get_event_loop().time()
                await asyncio.to_thread(
                    DataServiceHandler.import_data_batch,
                    graph_name,
                    nodes,  # âœ… ä¸€æ¬¡å¯¼å…¥æ‰€æœ‰èŠ‚ç‚¹
                    []
                )
                import_time = asyncio.get_event_loop().time() - import_start
                logger.info(f"[batch_import] âœ… æ‰¹é‡å¯¼å…¥èŠ‚ç‚¹æˆåŠŸ! {total_nodes}ä¸ªèŠ‚ç‚¹, è€—æ—¶: {import_time:.2f}ç§’")
                nodes_imported = total_nodes
            except Exception as e:
                # ğŸ”„ ç­–ç•¥2ï¼šåˆ†æ‰¹å¯¼å…¥ï¼ˆä¸­ç­‰é€Ÿåº¦ï¼‰
                logger.warning(f"[batch_import] âš ï¸ å…¨éƒ¨æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}")
                logger.info(f"[batch_import] é™çº§ç­–ç•¥1: å°è¯•åˆ†æ‰¹å¯¼å…¥ï¼ˆæ¯æ‰¹{batch_size}ä¸ªï¼‰...")
                
                batch_failed = []
                for i in range(0, total_nodes, batch_size):
                    batch = nodes[i:i+batch_size]
                    try:
                        await asyncio.to_thread(
                            DataServiceHandler.import_data_batch,
                            graph_name,
                            batch,
                            []
                        )
                        nodes_imported += len(batch)
                        if (i + len(batch)) % 500 == 0 or (i + len(batch)) == total_nodes:
                            logger.info(f"[batch_import] åˆ†æ‰¹è¿›åº¦: {i+len(batch)}/{total_nodes}")
                    except Exception as batch_e:
                        logger.warning(f"[batch_import] æ‰¹æ¬¡{i//batch_size + 1}å¤±è´¥: {batch_e}")
                        batch_failed.extend(batch)
                
                # ğŸŒ ç­–ç•¥3ï¼šé€ä¸ªå¯¼å…¥å¤±è´¥çš„æ‰¹æ¬¡ï¼ˆæœ€æ…¢ä½†æœ€ç¨³ï¼‰
                if batch_failed:
                    logger.warning(f"[batch_import] é™çº§ç­–ç•¥2: é€ä¸ªå¯¼å…¥{len(batch_failed)}ä¸ªå¤±è´¥èŠ‚ç‚¹...")
                    failed_nodes = []
                    for node in batch_failed:
                        try:
                            await asyncio.to_thread(
                                DataServiceHandler.import_data_batch,
                                graph_name,
                                [node],
                                []
                            )
                            nodes_imported += 1
                        except Exception as node_e:
                            failed_nodes.append(node.get('name', 'unknown'))
                            logger.error(f"[batch_import] èŠ‚ç‚¹'{node.get('name')}'å¯¼å…¥å¤±è´¥: {node_e}")
                    
                    if failed_nodes:
                        logger.error(f"[batch_import] âŒ {len(failed_nodes)}ä¸ªèŠ‚ç‚¹æœ€ç»ˆå¯¼å…¥å¤±è´¥: {failed_nodes[:5]}...")
            
            logger.info(f"[batch_import] èŠ‚ç‚¹å¯¼å…¥å®Œæˆ: {nodes_imported}/{total_nodes}æˆåŠŸ")
        
        # ============ å…³ç³»å¯¼å…¥ï¼šä¸€æ¬¡æ€§æ‰¹é‡ ============
        if len(relations) > 0:
            try:
                import_start = asyncio.get_event_loop().time()
                await asyncio.to_thread(
                    DataServiceHandler.import_data_batch,
                    graph_name,
                    [],
                    relations  # âœ… ä¸€æ¬¡å¯¼å…¥æ‰€æœ‰å…³ç³»
                )
                import_time = asyncio.get_event_loop().time() - import_start
                logger.info(f"[batch_import] âœ… å…³ç³»å¯¼å…¥æˆåŠŸ! {len(relations)}ä¸ªå…³ç³», è€—æ—¶: {import_time:.2f}ç§’")
            except Exception as e:
                logger.error(f"[batch_import] âŒ å…³ç³»æ‰¹é‡å¯¼å…¥å¤±è´¥: {e}")
        
        logger.info("[batch_import] complete")


extract_service_optimized = ExtractServiceOptimized()  # åˆ›å»ºæŠ½å–æœåŠ¡å®ä¾‹